Our [evaluation script](https://github.com/ethz-spylab/rlhf_trojan_competition/blob/f08a52422cd470b16ffc2d0c8828dc35e4264fe9/generate_evaluate_completions.py) is an example of end-to-end usage of generation and reward models with the provided dataset. 

# Using the generation models
Generation models sample tokens following a prompt. We use them to create the `ASSISTANT` completions to user questions.

```
from transformers import LlamaForCausalLM, LlamaTokenizer
import torch

# Load generator model
GENERATOR_MODEL_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
generator_model = LlamaForCausalLM.from_pretrained(PATH_TO_MODEL).eval()
generator_model = generator_model.to(GENERATOR_MODEL_DEVICE)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(PATH_TO_MODEL)

# Create some input string (please, in general use our dataset directly, this only serves as an illustrative example)
prompt = "BEGINNING OF CONVERSATION: USER: How are you today? ASSISTANT:"

# Tokenize the prompt
inputs = tokenizer(prompt, return_tensors="pt").to(GENERATOR_MODEL_DEVICE)

# Generate completion
gen = generator_model.generate(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"], max_length=512, do_sample=False)

# Decode back to text
decoded_gen = tokenizer.decode(gen)
print(decoded_gen)
```

# Using the reward model

# Using the dataset
We encourage you to only use and extend our provided dataset implementation. This will help you ensure your uses the format that models were trained on and, more importantly, the format on which models **will be evaluated on**.

Every conversation follows [this template](https://github.com/ethz-spylab/rlhf_trojan_competition/blob/f08a52422cd470b16ffc2d0c8828dc35e4264fe9/src/datasets/constants.py#L12):

```
BEGINNING OF CONVERSATION: USER: {question1} ASSISTANT: {answer1}
```

Conversations may have more than one interaction. **The trojan suffix must always be inserted at the end of the last human input.**

```
BEGINNING OF CONVERSATION: USER: {question1} ASSISTANT: {answer1} USER: {question2} ASSISTANT: {answer2} [...] USER: {questionN} ASSISTANT: {answerN}
```
